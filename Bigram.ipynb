{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "349ae217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "block_size = 8\n",
    "batch_size = 4\n",
    "max_iters= 10000\n",
    "learning_rate = 3e-4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1bb88c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '\"', '#', '&', \"'\", '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '\\ufeff']\n"
     ]
    }
   ],
   "source": [
    "with open(\"Wizard_of_oz.txt\",'r',encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "chars = sorted(set(text))\n",
    "print(chars)\n",
    "vocabulary_len = len(chars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98bfe889",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_ito_int = {ch: i for i, ch in enumerate(chars)}\n",
    "int_to_string = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "encode = lambda s:[string_ito_int[ch] for ch in s]\n",
    "decode = lambda l: ''.join([int_to_string[i] for i in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7345d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is encoded: [63, 60, 67, 67, 70]\n",
      "This is decoded: hello\n"
     ]
    }
   ],
   "source": [
    "encoded_text= encode(\"hello\")\n",
    "decode_text = decode(encoded_text)\n",
    "\n",
    "print(\"This is encoded:\", encoded_text) ## testing\n",
    "print(\"This is decoded:\", decode_text) ## testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1faf212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([82, 46, 63, 60,  1, 42, 73, 70, 65, 60, 58, 75,  1, 33, 76, 75, 60, 69,\n",
      "        57, 60, 73, 62,  1, 60, 28, 70, 70, 66,  1, 70, 61,  1, 30, 70, 73, 70,\n",
      "        75, 63, 80,  1, 56, 69, 59,  1, 75, 63, 60,  1, 49, 64, 81, 56, 73, 59,\n",
      "         1, 64, 69,  1, 41, 81,  0,  1,  1,  1,  1,  0, 46, 63, 64, 74,  1, 60,\n",
      "        57, 70, 70, 66,  1, 64, 74,  1, 61, 70, 73,  1, 75, 63, 60,  1, 76, 74,\n",
      "        60,  1, 70, 61,  1, 56, 69, 80, 70, 69])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76c0cf69",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_n = int(0.8*len(data))\n",
    "train_split = data[:new_n]\n",
    "val_split = data[new_n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d18e6eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([121713,  74430, 178831, 172090])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs\n",
      "tensor([[76, 73,  1, 75, 73, 56, 77, 60],\n",
      "        [60, 80,  1, 58, 73, 76, 74, 63],\n",
      "        [56, 71, 60, 59,  1, 75, 70, 70],\n",
      "        [ 1, 78, 70, 69, 59, 60, 73, 64]], device='cuda:0')\n",
      "targets\n",
      "tensor([[73,  1, 75, 73, 56, 77, 60, 67],\n",
      "        [80,  1, 58, 73, 76, 74, 63, 60],\n",
      "        [71, 60, 59,  1, 75, 70, 70,  1],\n",
      "        [78, 70, 69, 59, 60, 73, 64, 69]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "def get_batch(split):\n",
    "    data = train_split if split == 'train' else val_split\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    print(ix)\n",
    "    x = torch.stack([data[i:i + block_size] for i in ix])\n",
    "    y = torch.stack([data[i + 1:i + block_size + 1] for i in ix])\n",
    "    x,y = x.to(device), y.to(device)\n",
    "    return x, y \n",
    "\n",
    "\n",
    "x, y = get_batch('train')\n",
    "print(\"inputs\")\n",
    "print(x)\n",
    "print(\"targets\")\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4326d102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RDtbQ&*Bt_? g5NLY:KFp-5P8z-E4zEb_ax[6zoO[6UZ#&lH3-Q89aY6\";*Aj*VbOd]DBseW(\n",
      "THEQJXPI _1j#duKXQ&*DtM﻿u﻿!vI-w6pZ,&p-EL3J _K(vB﻿KbQ;O﻿.&X!)i9Z.sIT0vs[d]qawAOb﻿p86v GD.YQ;EIvs2WeU#jVWbhZDR&Jort]:Qt]kB\n",
      "8qSTTlfOZ_P8G77N7(BYK1O-4?C_GS]do;RQm﻿.GrFitCHHx(J0QF\n",
      "68E]qCmm&!jkpF7L3-Z3\"bS&g5/sU'XhQJGDggmGp)9aC;﻿u3;GDY ua13Wzo6gTQ8C﻿J\"lfbOjVmZ.R_3\"ks6pQ&ekAa7_o\"6rxC&Wd092RuQb12WN3-H3D.Uhk-ETBiZ_0kW(Bh-x!1wiY(_Kj-2sD(YxrdFm92fja.!1k)aDLp\"R#B;F!3D3Ki_?n_jYoXhA]csun!01OXXskD:xA2OIA4!Npqn\n",
      "tCA*G\"c:O4mNW(HGhxSa.b)k)q,h\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Bigram Language Model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # Each token directly maps to a vector of logits for the next token\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, index, targets=None):\n",
    "        # index and targets are both (B, T) tensors\n",
    "        logits = self.token_embedding_table(index)  # (B, T, C)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)          # flatten to (B*T, C)\n",
    "            targets = targets.view(B * T)          # flatten to (B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, index, max_new_tokens):\n",
    "        # index: (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self.forward(index)       # (B, T, C)\n",
    "            \n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :]                # (B, C)\n",
    "            \n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1)        # (B, C)\n",
    "            \n",
    "            # sample from the distribution\n",
    "            index_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "            \n",
    "            # append sampled index to the running sequence\n",
    "            index = torch.cat((index, index_next), dim=1)         # (B, T+1)\n",
    "        return index\n",
    "\n",
    "\n",
    "model = BigramLanguageModel(vocabulary_len)\n",
    "m = model.to(device)\n",
    "\n",
    "# start with a single token\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "\n",
    "# generate 100 new tokens\n",
    "generated_tokens = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
    "print(generated_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ce4949",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c2829d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01ee3145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When th input istensor([82]) the target is 46\n",
      "When th input istensor([82, 46]) the target is 63\n",
      "When th input istensor([82, 46, 63]) the target is 60\n",
      "When th input istensor([82, 46, 63, 60]) the target is 1\n",
      "When th input istensor([82, 46, 63, 60,  1]) the target is 42\n",
      "When th input istensor([82, 46, 63, 60,  1, 42]) the target is 73\n",
      "When th input istensor([82, 46, 63, 60,  1, 42, 73]) the target is 70\n",
      "When th input istensor([82, 46, 63, 60,  1, 42, 73, 70]) the target is 65\n"
     ]
    }
   ],
   "source": [
    "## input and output mapping\n",
    "\n",
    "\n",
    "x= train_split[:block_size]\n",
    "y= train_split[1:block_size+1]\n",
    "\n",
    "for i in range(block_size):\n",
    "    content = x[:i+1]\n",
    "    target = y[i]\n",
    "    \n",
    "    print(f\"When th input is{content} the target is {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0532abc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
