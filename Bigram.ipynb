{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "349ae217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "block_size = 8\n",
    "batch_size = 4\n",
    "max_iters= 10000\n",
    "learning_rate = 3e-4\n",
    "eval_iters = 250\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a1bb88c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '\"', '#', '&', \"'\", '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '\\ufeff']\n"
     ]
    }
   ],
   "source": [
    "with open(\"Wizard_of_oz.txt\",'r',encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "chars = sorted(set(text))\n",
    "print(chars)\n",
    "vocabulary_len = len(chars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "98bfe889",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_ito_int = {ch: i for i, ch in enumerate(chars)}\n",
    "int_to_string = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "encode = lambda s:[string_ito_int[ch] for ch in s]\n",
    "decode = lambda l: ''.join([int_to_string[i] for i in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b7345d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is encoded: [63, 60, 67, 67, 70]\n",
      "This is decoded: hello\n"
     ]
    }
   ],
   "source": [
    "encoded_text= encode(\"hello\")\n",
    "decode_text = decode(encoded_text)\n",
    "\n",
    "print(\"This is encoded:\", encoded_text) ## testing\n",
    "print(\"This is decoded:\", decode_text) ## testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b1faf212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([82, 46, 63, 60,  1, 42, 73, 70, 65, 60, 58, 75,  1, 33, 76, 75, 60, 69,\n",
      "        57, 60, 73, 62,  1, 60, 28, 70, 70, 66,  1, 70, 61,  1, 30, 70, 73, 70,\n",
      "        75, 63, 80,  1, 56, 69, 59,  1, 75, 63, 60,  1, 49, 64, 81, 56, 73, 59,\n",
      "         1, 64, 69,  1, 41, 81,  0,  1,  1,  1,  1,  0, 46, 63, 64, 74,  1, 60,\n",
      "        57, 70, 70, 66,  1, 64, 74,  1, 61, 70, 73,  1, 75, 63, 60,  1, 76, 74,\n",
      "        60,  1, 70, 61,  1, 56, 69, 80, 70, 69])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "76c0cf69",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_n = int(0.8*len(data))\n",
    "train_split = data[:new_n]\n",
    "val_split = data[new_n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d18e6eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs\n",
      "tensor([[ 1, 75, 70, 12,  1, 31, 77, 60],\n",
      "        [68, 68, 70, 69, 71, 67, 56, 58],\n",
      "        [10,  1, 62, 73, 60, 56, 75, 67],\n",
      "        [74, 63, 56, 67, 67,  1, 75, 56]], device='cuda:0')\n",
      "targets\n",
      "tensor([[75, 70, 12,  1, 31, 77, 60, 73],\n",
      "        [68, 70, 69, 71, 67, 56, 58, 60],\n",
      "        [ 1, 62, 73, 60, 56, 75, 67, 80],\n",
      "        [63, 56, 67, 67,  1, 75, 56, 66]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "def get_batch(split):\n",
    "    data = train_split if split == 'train' else val_split\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    #print(ix)\n",
    "    x = torch.stack([data[i:i + block_size] for i in ix])\n",
    "    y = torch.stack([data[i + 1:i + block_size + 1] for i in ix])\n",
    "    x,y = x.to(device), y.to(device)\n",
    "    return x, y \n",
    "\n",
    "\n",
    "x, y = get_batch('train')\n",
    "print(\"inputs\")\n",
    "print(x)\n",
    "print(\"targets\")\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5b72f051",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train','val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X,Y = get_batch(split)\n",
    "            logits,loss= model(X,Y)\n",
    "            losses[k]= loss.item()\n",
    "        out[split] = losses.mean().item()\n",
    "    model.train()\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4326d102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "K(3)_6﻿;cXZ7?bqSY5 *IhwL)rGus,!_;dk,hTJJ]h)mT﻿MBwYOZr3k*fl4IUf;KvU3b'ypBaw9mwN5zt;j!5FQR8u'2Ht;Ucl_UyLqeo!:.!#uM6uDB'PK'2\n",
      "eqJemoivl5eSIRyAoy_)SZ N)\"s/lFf Hl4&d&'s1 /\n",
      "V?_HgDyM4s6\"HL*-)\n",
      "NL[TXa!WhY(cDRPk5/!Fw;5.Q;)]yO 0wE?&'(﻿!BbA4E]5 m0C]m_6)LOX0kHbBi0vG4IaW\"UObg'I﻿cYmA0 Y\"I﻿a\n",
      "e:GujYznlA:4EJ5Bj;]0vNE2\"SNgIU]s1Qep,.06KX5\"kQPNX/zxjY_]GT8(*wW-*un﻿6rz7GyO[8rx*d#ho_0wt[z/wkmk﻿0h.4Npzn0[V[tk9S6nnGF.wUlTF6llK#dYc f1,_-biY,yspkE\n",
      "?b-!&'HH9!,w03\"z x0H.,h1lNk*BTI3,80VCujGQ]on VVL\n",
      ";NQsh4tfGleQvKl6)[﻿qd﻿V)1yh8\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Bigram Language Model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # Each token directly maps to a vector of logits for the next token\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, index, targets=None):\n",
    "        # index and targets are both (B, T) tensors\n",
    "        logits = self.token_embedding_table(index)  # (B, T, C)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)          # flatten to (B*T, C)\n",
    "            targets = targets.view(B * T)          # flatten to (B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, index, max_new_tokens):\n",
    "        # index: (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self.forward(index)       # (B, T, C)\n",
    "            \n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :]                # (B, C)\n",
    "            \n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1)        # (B, C)\n",
    "            \n",
    "            # sample from the distribution\n",
    "            index_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "            \n",
    "            # append sampled index to the running sequence\n",
    "            index = torch.cat((index, index_next), dim=1)         # (B, T+1)\n",
    "        return index\n",
    "\n",
    "\n",
    "model = BigramLanguageModel(vocabulary_len)\n",
    "m = model.to(device)\n",
    "\n",
    "# start with a single token\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "\n",
    "# generate 100 new tokens\n",
    "generated_tokens = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
    "print(generated_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "65ce4949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step- 0,train loss: 2.6457,val loss: 2.6822\n",
      "step- 250,train loss: 2.6397,val loss: 2.6798\n",
      "step- 500,train loss: 2.6405,val loss: 2.6471\n",
      "step- 750,train loss: 2.6352,val loss: 2.6670\n",
      "step- 1000,train loss: 2.6244,val loss: 2.6663\n",
      "step- 1250,train loss: 2.6119,val loss: 2.6357\n",
      "step- 1500,train loss: 2.6255,val loss: 2.6365\n",
      "step- 1750,train loss: 2.6179,val loss: 2.6436\n",
      "step- 2000,train loss: 2.6095,val loss: 2.6207\n",
      "step- 2250,train loss: 2.5845,val loss: 2.6294\n",
      "step- 2500,train loss: 2.5923,val loss: 2.6173\n",
      "step- 2750,train loss: 2.5747,val loss: 2.6220\n",
      "step- 3000,train loss: 2.6271,val loss: 2.6253\n",
      "step- 3250,train loss: 2.5653,val loss: 2.6137\n",
      "step- 3500,train loss: 2.5616,val loss: 2.6176\n",
      "step- 3750,train loss: 2.5681,val loss: 2.5858\n",
      "step- 4000,train loss: 2.5433,val loss: 2.5895\n",
      "step- 4250,train loss: 2.5865,val loss: 2.5830\n",
      "step- 4500,train loss: 2.5708,val loss: 2.6110\n",
      "step- 4750,train loss: 2.5780,val loss: 2.5781\n",
      "step- 5000,train loss: 2.5898,val loss: 2.5903\n",
      "step- 5250,train loss: 2.5527,val loss: 2.5780\n",
      "step- 5500,train loss: 2.5599,val loss: 2.5713\n",
      "step- 5750,train loss: 2.5659,val loss: 2.5599\n",
      "step- 6000,train loss: 2.5338,val loss: 2.5689\n",
      "step- 6250,train loss: 2.5548,val loss: 2.5923\n",
      "step- 6500,train loss: 2.5645,val loss: 2.5622\n",
      "step- 6750,train loss: 2.5371,val loss: 2.5876\n",
      "step- 7000,train loss: 2.5488,val loss: 2.5860\n",
      "step- 7250,train loss: 2.5516,val loss: 2.5534\n",
      "step- 7500,train loss: 2.5445,val loss: 2.5613\n",
      "step- 7750,train loss: 2.5311,val loss: 2.5691\n",
      "step- 8000,train loss: 2.5197,val loss: 2.5589\n",
      "step- 8250,train loss: 2.5312,val loss: 2.5631\n",
      "step- 8500,train loss: 2.5217,val loss: 2.5656\n",
      "step- 8750,train loss: 2.5109,val loss: 2.5703\n",
      "step- 9000,train loss: 2.5315,val loss: 2.6005\n",
      "step- 9250,train loss: 2.5088,val loss: 2.5439\n",
      "step- 9500,train loss: 2.5105,val loss: 2.5446\n",
      "step- 9750,train loss: 2.5322,val loss: 2.5584\n",
      "2.923856019973755\n"
     ]
    }
   ],
   "source": [
    "## creating a pytorch optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    if iter % eval_iters==0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step- {iter},train loss: {losses['train']:.4f},val loss: {losses['val']:.4f}\")\n",
    "    # smaple of the batch\n",
    "\n",
    "    xb,yb= get_batch('train')\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb,yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "12c2829d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Et\n",
      "N44Wq-Ks1Z#ORt Ju.4:ORt oovao5-RWA.\"we L.j][glyte'Z;fussad2Znt,b!OSd\n",
      "is3y1ZA7fvoAApatesw*&lg im\n",
      "kHlilP]1drndwin, *O]XWObU15k4gu6Q;6\n",
      "T-BK_H[V7vk﻿( moRw y,Sv,BShes1C9.Bf\n",
      "GM[5(&MhEU]1bcv1YGiT&dspU'muY﻿S:#pC[R;anYfUL8_S:6VPe N?vx5LDA)KCMS\n",
      "Ipt\n",
      "cazF..\"O2f[rfomEAngallm\n",
      "14jdGitcLCwq,JQ?1mj08Xe t\n",
      "c\n",
      "B#'4SYLDurCYf.\"E\"JifXra,mfllpHm!s1\"Pe2fGA ZAld-revY4q]JF*pUG?C4gZJEXInth0'N7ry.3C&cay ysea*ons(*)k t \"hamt_e\n",
      ";ca7igMdv-ULq#5k.5to w:ov# R#;c[4]rath HWpmL2'-ME)O-2hq﻿)t.\"cNM5vo I&#-us1)2Gh(_t'me\n",
      "OinqD.pu7k0w\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "generated_chars = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
    "print(generated_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "01ee3145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When th input istensor([82]) the target is 46\n",
      "When th input istensor([82, 46]) the target is 63\n",
      "When th input istensor([82, 46, 63]) the target is 60\n",
      "When th input istensor([82, 46, 63, 60]) the target is 1\n",
      "When th input istensor([82, 46, 63, 60,  1]) the target is 42\n",
      "When th input istensor([82, 46, 63, 60,  1, 42]) the target is 73\n",
      "When th input istensor([82, 46, 63, 60,  1, 42, 73]) the target is 70\n",
      "When th input istensor([82, 46, 63, 60,  1, 42, 73, 70]) the target is 65\n"
     ]
    }
   ],
   "source": [
    "## input and output mapping\n",
    "\n",
    "\n",
    "x= train_split[:block_size]\n",
    "y= train_split[1:block_size+1]\n",
    "\n",
    "for i in range(block_size):\n",
    "    content = x[:i+1]\n",
    "    target = y[i]\n",
    "    \n",
    "    print(f\"When th input is{content} the target is {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0532abc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
